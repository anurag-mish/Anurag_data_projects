{"cells": [{"cell_type": "code", "execution_count": 1, "id": "e1567136-6fef-4ed2-9e5e-9c04186fcd82", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "ProjectID (and not the Project Name) is: amishr96-cis415-2025springc\nBucket name is: amishr96_data_for_gcp_labs\n"}], "source": "# PySpark Pearson correlation function for distributed data processing\n\n# Access configuration to GCP Cloud Storage\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, avg, sqrt\nfrom pyspark.sql.types import StructType, StructField, FloatType, IntegerType, DoubleType\nfrom scipy.stats import pearsonr\nfrom itertools import combinations\n\n# 1. Configure the Project ID (not Project Name!!!) as per your GCP Dataproc setup\nproject_id = 'amishr96-cis415-2025springc'\n\n# 2. Configure Bucket name as per your Google Cloud Storage setup\nbucket = 'amishr96_data_for_gcp_labs'\n\n# 3. Configure the path to the movie reviews data file as per your Google Cloud Storage setup\npath_to_data_files = \"/data_for_assignment/\"\nmovie_reviews_file_name = \"movie_ratings (1).csv\"\nrelative_path_to_file = path_to_data_files[1:] + movie_reviews_file_name\nfull_file_path = \"gs://\" + bucket + \"/\" + relative_path_to_file\n\nprint(f\"ProjectID (and not the Project Name) is: {project_id}\")\nprint(f\"Bucket name is: {bucket}\")"}, {"cell_type": "code", "execution_count": 2, "id": "467a527b-b55a-488d-9d0e-7f50778e43b4", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "PySpark already pre-installed!\n"}], "source": "# Typically, any big data platform (like GCP Dataproc) will have PySpark pre-installed\n# This might not be the case in other platforms.\n# This paragraph is to check if PySpark is available in the system and install if it's not available\n# You should expect this paragraph to RUN the PySpark installation in Google Colab\n# You should expect this paragraph NOT TO RUN the PySpark installation in GCP Dataproc\n\ntry:\n  from pyspark.sql import SparkSession\n  pyspark_available = 'Y'\nexcept:\n  pyspark_available = 'N'\n\n# If PySpark is not installed, then go through all these steps\n\nif pyspark_available == 'N':\n  # Update Installer\n  !apt-get update\n\n  # Intsall Java\n  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n\n  # install spark (change the version number if needed)\n  !wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n\n  # unzip the spark file to the current folder\n  !tar xf spark-3.0.0-bin-hadoop3.2.tgz\n\n  # set your spark folder to your system path environment.\n  import os\n  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n  os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n\n  # install findspark using pip\n  !pip install -q findspark\n\n  import findspark\n  findspark.init()\n\nelse:\n    # Spark / PySpark already pre-installed in the environment\n    print(\"PySpark already pre-installed!\")\n"}, {"cell_type": "code", "execution_count": 6, "id": "feea8853-db09-41f5-add0-982c70501a6a", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Not running on CoLab\nPackage imports done\nReading the file using spark directly from GCS\n"}, {"name": "stderr", "output_type": "stream", "text": "25/04/11 22:58:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+---------+------------------+------------------+------------------+------------------+\n|summary|     Name|         Inception|           Titanic|            Avatar|        The Matrix|\n+-------+---------+------------------+------------------+------------------+------------------+\n|  count|      100|               100|               100|               100|               100|\n|   mean|     NULL| 3.113999999999999|2.9800000000000004|3.1839999999999997|2.9620000000000006|\n| stddev|     NULL|1.1372499170091743|1.2192894105447922|1.1451893231507992|1.0880229498470788|\n|    min|Alexander|               1.0|               1.0|               1.0|               1.0|\n|    max|  William|               5.0|               5.0|               4.9|               4.9|\n+-------+---------+------------------+------------------+------------------+------------------+\n\nTotal number of records from data file = 100\n"}], "source": "# Check if the code is running in Colab\nif 'google.colab' in str(get_ipython()):\n  print('Running on CoLab')\n  RunningInColab = True\nelse:\n  print('Not running on CoLab')\n  RunningInColab = False# Authorize access from Colab to GCP Cloud Storage\nfrom google.cloud import storage\nclient = storage.Client()\nprint(f\"Package imports done\")\n\nif RunningInColab == True:\n  # To access Google Cloud Storage\n  from google.cloud import storage\n  import google.auth\n\n  !pip install gcsfs\n  import gcsfs\n\n  from google.colab import auth\n  auth.authenticate_user()\n\n  credentials, default_project_id = google.auth.default()\n  !gcloud config set project {project_id}# Creat a Spark session if in Google Colab\nfrom pyspark.sql import SparkSession\n\nif RunningInColab == True:\n    spark = SparkSession.builder.master(\"local[*]\").getOrCreate()# READ DATA\n\nif RunningInColab == True:\n  # Currently, it's not easy to read files from Google Storage using spark.read.csv in Google Colab.\n  # As a workaround, we will copy the file from Google Storage to a local file in the runtime machine on which Colab is running.\n\n  # Initialize a Google Cloud Storage client\n  storage_client = storage.Client()\n\n  # Download the file from GCS to a local file in the default folder \"/content\" in the runtime machine of colab\n  temp_file_path = \"/content/\"+ movie_reviews_file_name\n\n  print(f\"bucket_name={bucket}\")\n  print(f\"relative_path_to_file = {relative_path_to_file}\")\n  print(f\"Downloading the file from GCS to a local file in Colab\")\n  bucket_object = storage_client.bucket(bucket)\n  print(bucket_object.name)\n  blob = bucket_object.blob(relative_path_to_file)\n  print(blob)\n  blob.download_to_filename(temp_file_path)\n\n  print(f\"Reading the local file using spark\")\n  # Read the file using the local temporary path\n  spark_df = spark.read.csv(temp_file_path, sep=\",\", header=True)\nelse:\n  #In GCP, we can read the file directly from Google Storage\n  print(f\"Reading the file using spark directly from GCS\")\n  spark_df = spark.read.csv(full_file_path, sep=\",\", header=True, inferSchema = True)\n\nspark_df.describe().show()\n\n# How many records got loaded?\nprint(f\"Total number of records from data file = {spark_df.count()}\")\n\n"}, {"cell_type": "code", "execution_count": 7, "id": "df6103b5-5d9b-4e07-be1e-a26e28971b49", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Name: string (nullable = true)\n |-- Inception: double (nullable = true)\n |-- Titanic: double (nullable = true)\n |-- Avatar: double (nullable = true)\n |-- The Matrix: double (nullable = true)\n\n"}], "source": "spark_df.printSchema()"}, {"cell_type": "code", "execution_count": 8, "id": "286108c0-75c9-489d-9dba-6d326dad90cd", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Name: string (nullable = true)\n |-- Inception: double (nullable = true)\n |-- Titanic: double (nullable = true)\n |-- Avatar: double (nullable = true)\n |-- The Matrix: double (nullable = true)\n\n"}], "source": "spark_df = spark_df.withColumn(\"Inception\", col(\"Inception\").cast(DoubleType()))\nspark_df = spark_df.withColumn(\"Titanic\", col(\"Titanic\").cast(DoubleType()))\nspark_df = spark_df.withColumn(\"Avatar\", col(\"Avatar\").cast(DoubleType()))\nspark_df = spark_df.withColumn(\"The Matrix\", col(\"The Matrix\").cast(DoubleType()))\nspark_df.printSchema()"}, {"cell_type": "code", "execution_count": 9, "id": "f9a351b6-1997-4206-adb2-8c0a2057d54b", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Collect the ratings as a dictionary\nratings_dict = spark_df.rdd.map(lambda row: (row[\"Name\"], [row[\"Inception\"], row[\"Titanic\"], row[\"Avatar\"], row[\"The Matrix\"]\n                                                          ])).collectAsMap()\n"}, {"cell_type": "code", "execution_count": 10, "id": "d4ea7495-8cb3-4e7d-848f-5fd12eb248e5", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Rachel: [4.5, 2.3, 3.6, 3.0]\nLarry: [2.1, 4.2, 2.9, 3.3]\nEdward: [4.7, 1.1, 4.8, 3.4]\nGary: [4.2, 2.2, 4.4, 3.8]\nDebra: [1.8, 4.2, 3.9, 2.7]\n"}], "source": "# Display a few entries in the dictionary\nfor key, value in list(ratings_dict.items())[:5]:\n    print(f\"{key}: {value}\")"}, {"cell_type": "code", "execution_count": 11, "id": "11470d50-03d9-4222-a477-6eb0ae979378", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[('Rachel', 'Larry'), ('Rachel', 'Edward'), ('Rachel', 'Gary'), ('Rachel', 'Debra'), ('Rachel', 'Carol')]\n"}], "source": "# Generate all pairs of users (user_name strings)\nuser_pairs = list(combinations(ratings_dict.keys(), 2))\n\nprint(user_pairs[:5])\n"}, {"cell_type": "code", "execution_count": 13, "id": "ce67c390-c23a-46a8-af54-b42dafbed64e", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "All unique users involved in results: ['Alexander', 'Amanda', 'Amy', 'Andrew', 'Angela', 'Anna', 'Anthony', 'Ashley', 'Barbara', 'Benjamin', 'Betty', 'Brandon', 'Brenda', 'Brian', 'Carol', 'Carolyn', 'Catherine', 'Charles', 'Christine', 'Christopher', 'Cynthia', 'Daniel', 'David', 'Deborah', 'Debra', 'Dennis', 'Donald', 'Donna', 'Dorothy', 'Edward', 'Elizabeth', 'Emily', 'Emma', 'Eric', 'Frank', 'Gary', 'George', 'Gregory', 'Helen', 'Jack', 'Jacob', 'James', 'Janet', 'Jason', 'Jeffrey', 'Jennifer', 'Jerry', 'Jessica', 'John', 'Jonathan', 'Joseph', 'Joshua', 'Justin', 'Karen', 'Katherine', 'Kathleen', 'Kenneth', 'Kevin', 'Kimberly', 'Larry', 'Laura', 'Linda', 'Lisa', 'Margaret', 'Maria', 'Mark', 'Mary', 'Matthew', 'Melissa', 'Michael', 'Michelle', 'Nancy', 'Nicholas', 'Nicole', 'Pamela', 'Patricia', 'Patrick', 'Paul', 'Rachel', 'Raymond', 'Rebecca', 'Richard', 'Robert', 'Ronald', 'Ruth', 'Ryan', 'Samantha', 'Sandra', 'Sarah', 'Scott', 'Sharon', 'Shirley', 'Stephanie', 'Stephen', 'Steven', 'Susan', 'Thomas', 'Timothy', 'Tyler', 'William']\n[('Rachel', 'Larry', 0, 0.007544739840356529), ('Rachel', 'Edward', 0.8838821066510862, 0.11611789334891376), ('Rachel', 'Gary', 0.8197466197308976, 0.18025338026910243), ('Rachel', 'Debra', 0, 0.25564768544063154), ('Rachel', 'Carol', 0, 0.8895916860170703)]\n"}], "source": "results = []\nall_users = set()\n\nfor user1_name, user2_name in user_pairs:\n    ratings_user1 = ratings_dict[user1_name]\n    ratings_user2 = ratings_dict[user2_name]\n\n    try:\n        correlation, p_value = pearsonr(ratings_user1, ratings_user2)\n        if correlation < 0:\n            correlation = 0\n        results.append((user1_name, user2_name, correlation, p_value))\n        all_users.update([user1_name, user2_name])\n    except Exception as e:\n        print(f\"Skipped pair ({user1_name}, {user2_name}) \u2014 Error: {e}\")\n\nprint(\"All unique users involved in results:\", sorted(all_users))\nprint(results[:5])\n"}, {"cell_type": "code", "execution_count": 14, "id": "a1b876f6-4e6c-457c-a660-e07c43ca4bdd", "metadata": {"tags": []}, "outputs": [], "source": "# Step 2: Obtain a list of correlation values for each user and sort them (the higher the correlation the greater the similarity)\n\nfrom collections import defaultdict\nuserDistances = defaultdict(dict)\n\n# Task: You will create a dictionary userDistances that has a nested dictionary. The structure looks like the follows:\n# {'key_userName': {'userName1': correlation between key_userName and userName1, 'userName2': correlation between key_userName and userName 2 ...},...}\n\n# Your Code Starts HERE #\nfor user1, user2, correlation, _ in results:\n    if user2 not in userDistances[user1]:\n        userDistances[user1][user2] = correlation\n    if user1 not in userDistances[user2]:\n        userDistances[user2][user1] = correlation\n\n# Your Code Ends HERE#\n\n"}, {"cell_type": "code", "execution_count": 17, "id": "bcef0d00-c48d-4bd3-b58d-546dcb7cbe0c", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{'Sarah': 0.9961078632130002, 'Gary': 0.9886431886948035, 'Kevin': 0.9663526844810868, 'George': 0.9449897664771802, 'Joshua': 0.9308308637436923, 'Benjamin': 0.8992607007433459, 'Andrew': 0.8939635544523927, 'Jerry': 0.8899349063134984, 'Rachel': 0.8838821066510862, 'Brenda': 0.845568645734488, 'Donald': 0.8434548437347882, 'Jessica': 0.8400107008414178, 'William': 0.8173539673956701, 'Sharon': 0.8138086039790637, 'Ronald': 0.810454081347384, 'James': 0.8065174884550839, 'Jennifer': 0.7848106364871035, 'Carolyn': 0.7780905296182808, 'Nancy': 0.7651052880275334, 'Elizabeth': 0.7395932274474748, 'Emily': 0.7265138939446737, 'David': 0.7107839379747571, 'Donna': 0.7070710723297837, 'Stephanie': 0.6896281675560765, 'Alexander': 0.6804225750007863, 'Kathleen': 0.6629984200623009, 'Timothy': 0.6625800588444851, 'Catherine': 0.6593026183333233, 'Jason': 0.6461300931386637, 'Thomas': 0.6322039809758101, 'Pamela': 0.5779344204000937, 'Christopher': 0.5739717912626814, 'Dennis': 0.5604124522860795, 'Anthony': 0.5492880622225543, 'Cynthia': 0.5289421115538018, 'Richard': 0.4810512755994206, 'Patrick': 0.4526917234566462, 'Rebecca': 0.4022409138923945, 'Frank': 0.39575632893984086, 'Shirley': 0.3567249624463391, 'Daniel': 0.3052408828168059, 'Charles': 0.28343160155688807, 'Matthew': 0.26372953482886397, 'Michelle': 0.23469087900319374, 'Amy': 0.2229446500192499, 'Robert': 0.14004965346974538, 'Angela': 0.13231511983379304, 'Sandra': 0.11843760242387183, 'Helen': 0.08709149181288499, 'Nicholas': 0.07118750729711106, 'Joseph': 0.06147987704036885, 'Katherine': 0.04378768505884536, 'Ruth': 0.042421611857721975, 'Larry': 0, 'Debra': 0, 'Carol': 0, 'Kimberly': 0, 'Christine': 0, 'Margaret': 0, 'Patricia': 0, 'Justin': 0, 'Jack': 0, 'Mark': 0, 'Brian': 0, 'Betty': 0, 'Barbara': 0, 'Stephen': 0, 'Anna': 0, 'Janet': 0, 'Michael': 0, 'Scott': 0, 'Ryan': 0, 'Amanda': 0, 'Eric': 0, 'Dorothy': 0, 'Ashley': 0, 'Kenneth': 0, 'Susan': 0, 'Deborah': 0, 'Karen': 0, 'Laura': 0, 'Linda': 0, 'Paul': 0, 'Tyler': 0, 'Samantha': 0, 'Emma': 0, 'Mary': 0, 'Jeffrey': 0, 'Maria': 0, 'John': 0, 'Lisa': 0, 'Jacob': 0, 'Raymond': 0, 'Nicole': 0, 'Jonathan': 0, 'Gregory': 0, 'Melissa': 0, 'Brandon': 0, 'Steven': 0}\nEdward's neighbors: {'Sarah': 0.9961078632130002, 'Gary': 0.9886431886948035, 'Kevin': 0.9663526844810868}\nThe three nearest neighbors and their distances to Edward are dict_items([('Sarah', 0.9961078632130002), ('Gary', 0.9886431886948035), ('Kevin', 0.9663526844810868)])\n"}], "source": "# Step 3 Determine each user's k Nearest Neighbors.\n\n\n#The k Nearest neighbors of each user are essentially the first k users in the sorted inner dictionary in the decendant order (the\n# higher the number the more similar).\n\nfor key, inner_dict in userDistances.items():\n    userDistances[key] = dict(sorted(inner_dict.items(), key = lambda item: item[1], reverse = True))\n\n#The print output below should show the key 'Edward' and the corresponding inner dictionary. The values in the inner dictionary should\n# be sorted. They are the correlations between Rachel and other users\n\nprint(userDistances[\"Edward\"])\n\nk = 3\n\n# Task: Now we only retain the first k users' information. Please note that we take not only their names but also their correlation coefficents with the user, as we will use them to\n# calculate weights later. You will create a dictionary userKNNDistances to store such information. In this dictionary, a user's name is a key and the value of this key is an inner dictionary {nearest neighbor 1: coefficent value; nearest neighbor 2: coefficient value; ...}\nuserKNNDistances = {}\n# Your Code Starts HERE\nfor key, inner_dict in userDistances.items():\n    top_k_neighbors = dict(list(inner_dict.items())[:k])\n    userKNNDistances[key] = top_k_neighbors\nprint(\"Edward's neighbors:\", userKNNDistances[\"Edward\"])\n\n# Your Code Ends HERE\n\nfor key, inner_dict in list(userKNNDistances.items()):\n    if (key == \"Edward\"):\n        print(f\"The three nearest neighbors and their distances to {key} are {inner_dict.items()}\")"}, {"cell_type": "code", "execution_count": 18, "id": "8730687b-54f0-4db5-bcac-6fd9a1eda783", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "The three nearnest neighbors of Edward and their weights for calculating Edward's predicted ratings are: {'Sarah': 0.33753739352852596, 'Gary': 0.3350079417758977, 'Kevin': 0.32745466469557644}\n"}], "source": "# Step 4 Compute the weights for the nearest neighbors based on their distances (pearson correlation) for each user\n# weight of one Nearest Neighbor (NN) = (distance of that NN with userX) / (sum of distances between each NN with the user)\n# e.g., for Rachel, the weight of Andrew would be 0.9996960054843319/(0.9996960054843319+0.9916728130040291+0.9826757036739959)\n\n# You will use the dictionary userWeightNeighbors to record the users' names (keys of this dictionary), and the three nearest neighbors and their weights (an inner dictionary, the neighbors' names are the keys)\n\nfrom collections import defaultdict\n\nuserWeightNeighbors = defaultdict(dict)\n\nfor key, inner_dict in userKNNDistances.items():\n    denominator = sum(inner_dict.values())\n\n    weights = {}\n    for neighbor, similarity in inner_dict.items():\n        weight = similarity / denominator if denominator > 0 else 0\n        weights[neighbor] = weight\n\n    userWeightNeighbors[key] = weights\n\n# Your Code ENDs HERE\n\nfor key, value in userWeightNeighbors.items():\n    if (key == \"Edward\"):\n        print(f\"The three nearnest neighbors of {key} and their weights for calculating {key}'s predicted ratings are: {value}\")\n\n\n"}, {"cell_type": "code", "execution_count": 19, "id": "2c7de17c-3bf1-4b6b-8f65-4ae83107d21c", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Reading the file using spark directly from GCS\n+-------+---------+------------------+\n|summary|     Name|Mission Impossible|\n+-------+---------+------------------+\n|  count|      100|               100|\n|   mean|     NULL|1.6710000000000003|\n| stddev|     NULL|1.6547632501785303|\n|    min|Alexander|               0.0|\n|    max|  William|               4.8|\n+-------+---------+------------------+\n\n"}], "source": "# Step 5 Suppose the new Mission Impossible movie just came out and some users have watched it but some are not.\n# Find out who you would recommend this movie to, assuming that the predicted rating should be 3 (of the 5 rating scale)\n\n# 5.1. READ THIS NEW DATA\n\nmovie_review_file_name = \"missionImpossible_rating (1).csv\"\nrelative_path_to_file = path_to_data_files[1:] + movie_review_file_name\nfull_file_path = \"gs://\" + bucket + \"/\" + relative_path_to_file\n\nif RunningInColab == True:\n  # Download the file from GCS to a local file in the default folder \"/content\" in the runtime machine of colab\n  temp_file_path = \"/content/\"+ movie_review_file_name\n    # Initialize a Google Cloud Storage client\n  storage_client = storage.Client()\n\n  # Download the file from GCS to a local file in the default folder \"/content\" in the runtime machine of colab\n  temp_file_path = \"/content/\"+ movie_review_file_name\n  print(f\"bucket_name={bucket}\")\n  print(f\"relative_path_to_file = {relative_path_to_file}\")\n  print(f\"Downloading the file from GCS to a local file in Colab\")\n  bucket_object = storage_client.bucket(bucket)\n  blob = bucket_object.blob(relative_path_to_file)\n  blob.download_to_filename(temp_file_path)\n  print(f\"Reading the local file using spark\")\n  # Read the file using the local temporary path\n  spark_recommendation_df = spark.read.csv(temp_file_path, sep=\",\", header=True)\nelse:\n  #In GCP, we can read the file directly from Google Storage\n  print(f\"Reading the file using spark directly from GCS\")\n  relative_path_to_file = path_to_data_files[1:] + movie_review_file_name\n  full_file_path_update = \"gs://\" + bucket + \"/\" + relative_path_to_file\n  spark_recommendation_df = spark.read.csv(full_file_path, sep=\",\", header=True, inferSchema = True)\n\nspark_recommendation_df.describe().show()\n"}, {"cell_type": "code", "execution_count": 20, "id": "21341c40-6806-49b6-bb16-71cfe9022cb5", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Name: string (nullable = true)\n |-- Mission Impossible: double (nullable = true)\n\n"}], "source": "spark_recommendation_df.printSchema()"}, {"cell_type": "code", "execution_count": 21, "id": "7c1025de-b386-4ef9-bfe3-b130baea9d05", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Name: string (nullable = true)\n |-- Mission Impossible: double (nullable = true)\n\n+------+------------------+\n|  Name|Mission Impossible|\n+------+------------------+\n|Rachel|               4.5|\n| Larry|               2.1|\n|Edward|               0.0|\n|  Gary|               4.2|\n| Debra|               0.0|\n+------+------------------+\nonly showing top 5 rows\n\n"}], "source": "# Task: If the value for Mission Impossible is a string type, we need to convert it to a Double Type\n# (Hint: we have done this earlier in this code file for a different data file)\n\n# Your Code Starts Here\nfrom pyspark.sql.functions import col\n\n# Convert the Mission Impossible column to double\nspark_recommendation_df = spark_recommendation_df.withColumn(\n    \"Mission Impossible\", col(\"Mission Impossible\").cast(\"double\")\n)\n\n# Confirm schema change\nspark_recommendation_df.printSchema()\n\n# Optional: show cleaned data\nspark_recommendation_df.show(5)\n\n# Your Code Ends Here"}, {"cell_type": "code", "execution_count": 22, "id": "43780ee0-36e7-429b-a0ec-b92bda0f331d", "metadata": {"tags": []}, "outputs": [], "source": "# 5.2. Find out who has not watched it yet\n\n\n\n# First, collect the ratings as a dictionary\n\nrecommendation_dict = spark_recommendation_df.rdd.map(lambda row: (row[\"Name\"], [row[\"Mission Impossible\"]])).collectAsMap()\n\n\n"}, {"cell_type": "code", "execution_count": 23, "id": "83bd30f5-51c8-4b10-9f7d-fa8aabc6040a", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "['Edward', 'Debra', 'Carol', 'Christine', 'Nancy', 'Richard', 'Patricia', 'Jason', 'Thomas', 'Alexander', 'Mark', 'Anthony', 'Brian', 'Benjamin', 'Anna', 'Angela', 'Janet', 'Kathleen', 'Shirley', 'Dennis', 'Dorothy', 'Joshua', 'Carolyn', 'Andrew', 'Jessica', 'Deborah', 'Katherine', 'Emily', 'Timothy', 'Elizabeth', 'Paul', 'James', 'Tyler', 'Emma', 'William', 'Maria', 'John', 'Jacob', 'Raymond', 'Nicole', 'Gregory', 'Melissa', 'Steven']\n"}], "source": "# Next, find out those with values being 0 and put them in a list usersNotWatched\n\n# Your Code Starts HERE\n# Your Code Starts HERE\nusersNotWatched = (\n    spark_recommendation_df\n    .filter((col(\"Mission Impossible\").isNull()) | (col(\"Mission Impossible\") == 0.0))\n    .select(\"Name\")\n    .rdd.flatMap(lambda x: x)\n    .collect()\n)\n# Your Code Ends HERE\n\n# Your Code Ends HERE\n\nprint(usersNotWatched)"}, {"cell_type": "code", "execution_count": 24, "id": "68e17f63-50de-4f50-a245-f6dceea0023a", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "3.7793503003122257\n"}], "source": "# Now, let's find out from this list who we should recommend Mission Impossible, based on the criterion that the predicted\n# rating should be at least 3. Remember, predicted rating = sum (weight of each Nearest Neighbor * rating given by that neighbor)\n\npredictedRatings = {}\n\n# First, we calculate predicted ratings for all who have not watched the movie yet\nfor key in usersNotWatched:\n    predictedRatings[key] = 0\n    weights = userWeightNeighbors[key].values()\n    neighbors = userWeightNeighbors[key].keys()\n\n    # Your Code Starts HERE\n    for neighbor in neighbors:\n        neighbor_rating = spark_recommendation_df.filter(col(\"Name\") == neighbor).select(\"Mission Impossible\").collect()[0][0]\n        if neighbor_rating is not None:\n            neighbor_rating = float(neighbor_rating)  # ensure it\u2019s numeric\n            if neighbor_rating > 0:\n                weight = userWeightNeighbors[key][neighbor]\n                predictedRatings[key] += weight * neighbor_rating\n    # Your Code Ends HERE\n\n#Let's print out one from predictedRatings dictionary using Edward as the key\n\nprint(predictedRatings[\"Edward\"])\n"}, {"cell_type": "code", "execution_count": 25, "id": "a280e783-1135-4f63-b02a-f43f96239927", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "This is the list of users we will recommend the new movie Mission Impossible: ['Edward', 'Thomas', 'Andrew']\n"}], "source": "# From this predictedRatings dictionary, we will select those whose ratings is at least 3. We use a list forRecommendations to store\n# the list of these users\n\n# Your Code STARTS HERE\nforRecommendations = [user for user, rating in predictedRatings.items() if rating >= 3]\n\n# Your Code ENDS Here\n\nprint(f\"This is the list of users we will recommend the new movie Mission Impossible: {forRecommendations}\")"}, {"cell_type": "code", "execution_count": null, "id": "47724b5d-39b0-4af3-bfa1-da671c1bd780", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}